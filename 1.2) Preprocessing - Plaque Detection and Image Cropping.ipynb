{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Preprocessing - Plaque Detection and Image Cropping\n",
    "\n",
    "The previous dataset color normalizes the whole slide images and regularly tiles them to 1536 by 1536 pixel images. To generate the plaque dataset for expert annotation, we apply an HSV filter and smoothing to detect candidate plaques. Each candidate is center cropped to provide a 256 x 256 image, and bounding coordinates are generated and written to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob, os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'data/norm_tiles/'\n",
    "SAVE_DIR = 'data/seg/'\n",
    "\n",
    "BLOBS_DIR = SAVE_DIR + 'blobs/'\n",
    "IMG_BBOXES = SAVE_DIR + 'blobs_bboxes/'\n",
    "NEGATIVES = SAVE_DIR + 'negatives/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(imagepath, \n",
    "               figsize=(5,5)):\n",
    "    \"\"\"\n",
    "    Convenience function to plot images\n",
    "    loaded with OpenCV\n",
    "    (converts BGR2RGB)\n",
    "    \"\"\"\n",
    "    im  = cv2.imread(imagepath)\n",
    "    im2 = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(im2)\n",
    "    plt.show()\n",
    "    \n",
    "def hsv_mask(im,\n",
    "             hue=[0, 40],\n",
    "             sat=[10, 255],\n",
    "             val=[0, 220],\n",
    "             figsize=(5,5),\n",
    "             show=False):\n",
    "    \"\"\"\n",
    "    converts image to HSV colorspace,\n",
    "    applies color filters based on input range\n",
    "    and returns mask\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert to HSV colorspace\n",
    "    im2 = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "                       \n",
    "    # define mask ranges\n",
    "    lower = np.array([hue[0], sat[0], val[0]])\n",
    "    upper = np.array([hue[1], sat[1], val[1]])\n",
    "    mask = cv2.inRange(im2, lower, upper)\n",
    "    \n",
    "    if show:\n",
    "        im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        output = cv2.bitwise_and(im_rgb, im_rgb, mask = mask)\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('original')\n",
    "        plt.imshow(im_rgb)\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.title('mask ')\n",
    "        plt.imshow(mask)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.title('mask applied')\n",
    "        plt.imshow(output)\n",
    "        plt.show()\n",
    "    return mask\n",
    "\n",
    "def clean_mask(mask,\n",
    "               mask_threshold=0.25):\n",
    "    \"\"\"\n",
    "    function that applies opencv2 operations\n",
    "    to reduce noise and create smoother segments\n",
    "    \"\"\"\n",
    "    # define a kernel structure\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))\n",
    "    mask_nonzero = cv2.countNonZero(mask)\n",
    "    \n",
    "    # If the mask has a large amount of brown hue\n",
    "    # (above 20% of the mask), then apply an erosion\n",
    "    if mask_nonzero > mask_threshold * mask.size:\n",
    "        mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "    # Apply morphological closing, then opening operations \n",
    "    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    cleaned = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # If this doesn't result in a cleaner mask (fewer pixels)\n",
    "    # then the image is relatively \"dirty\" and we should apply\n",
    "    # opening before closing to reduce noise levels \n",
    "    if cv2.countNonZero(cleaned) > mask_nonzero:\n",
    "        opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        cleaned = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def show_bboxes(im, bboxes):\n",
    "    \"\"\"\n",
    "    draws bboxes on image object\n",
    "    \"\"\"\n",
    "    im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(121)\n",
    "    plt.title('original')\n",
    "    plt.imshow(im_rgb)\n",
    "    \n",
    "    for box in bboxes:\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "        x2 = box[0] + box[2]\n",
    "        y2 = box[1] + box[3]\n",
    "        cv2.rectangle(im_rgb, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "    plt.subplot(122)\n",
    "    plt.title('bounding boxes')\n",
    "    plt.imshow(im_rgb)\n",
    "    plt.show()\n",
    "    return im_rgb\n",
    "\n",
    "def crop_from_img(img,\n",
    "                  bboxes,\n",
    "                  size=(128,128),\n",
    "                  show=False):\n",
    "    \"\"\"\n",
    "    center crops of a fixed size from an image\n",
    "    bboxes: format is (x, y, w, h) where x,y is left, top\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    cropped_coords = []\n",
    "    for bbox in bboxes:\n",
    "        # get the x,y of the centerpoint of the bounding box\n",
    "        box_center_x = bbox[0] + 0.5 * bbox[2]\n",
    "        box_center_y = bbox[1] + 0.5 * bbox[3]\n",
    "\n",
    "        # ensure that the bounding box of the desired size\n",
    "        # stays within the shape of the image\n",
    "        centerpoint_x = np.clip(box_center_x, \n",
    "                                 a_min = size[0] / 2,\n",
    "                                 a_max = img.shape[0] - size[0] / 2)\n",
    "        centerpoint_y = np.clip(box_center_y, \n",
    "                                 a_min = size[1] / 2,\n",
    "                                 a_max = img.shape[1] - size[1] / 2)\n",
    "        \n",
    "        # top, left of crop box\n",
    "        # cast to int\n",
    "        x_crop = int(centerpoint_x - 0.5 * size[0])\n",
    "        y_crop = int(centerpoint_y - 0.5 * size[1])\n",
    "        \n",
    "        cropped_coords.append((x_crop, y_crop, size[0], size[1]))\n",
    "        crop_img = img[y_crop:y_crop + size[1],\n",
    "                       x_crop:x_crop + size[0]]\n",
    "        if show:\n",
    "            plt.figure()\n",
    "            im2 = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(im2)\n",
    "        cropped_images.append(crop_img)\n",
    "    return cropped_images, np.array(cropped_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_negative_bboxes(mask,\n",
    "                        pixel_threshold=500,\n",
    "                        img_size=(128,128),\n",
    "                        num_negatives=5,\n",
    "                        max_guesses=1000):\n",
    "    \"\"\"\n",
    "    Randomly samples a binary mask to produce\n",
    "    negative examples below a given threshold.\n",
    "    \"\"\"\n",
    "    mask = mask.copy()\n",
    "    negatives = []\n",
    "    \n",
    "    # Define the relevant sampling ranges\n",
    "    x_range = mask.shape[0] - img_size[0]\n",
    "    y_range = mask.shape[1] - img_size[1]\n",
    "    \n",
    "    iteration = 0\n",
    "    while len(negatives) < num_negatives and iteration < max_guesses:\n",
    "        # grab a random (x,y) point\n",
    "        x = np.random.randint(x_range)\n",
    "        y = np.random.randint(y_range)\n",
    "        # if the \n",
    "        random_crop = mask[y: y + img_size[0],\n",
    "                           x: x + img_size[1]]\n",
    "\n",
    "        nonzero = np.count_nonzero(random_crop)\n",
    "\n",
    "        if nonzero < pixel_threshold:\n",
    "            negatives.append((x, y, img_size[0], img_size[1]))\n",
    "            mask[y: y + img_size[0], x: x + img_size[1]] = 255 # prevent overlap by setting mask\n",
    "        iteration += 1\n",
    "    return np.array(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def watershed_mask(mask,\n",
    "                   img,\n",
    "                   dist_thresh=0.4,\n",
    "                   show=False):\n",
    "    \n",
    "    kernel = np.ones((8,8),np.uint8)\n",
    "    \n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    # sure background area\n",
    "    sure_bg = cv2.dilate(mask, kernel, iterations=3)\n",
    "    dist_transform = cv2.distanceTransform(mask,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform,\n",
    "                                 dist_thresh * dist_transform.max(),\n",
    "                                 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    # Marker labelling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown==255] = 0\n",
    "    markers = cv2.watershed(img, markers)\n",
    "    #img[markers == -1] = [255, 0, 0]\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(markers)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    return markers, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_and_bound(mask,\n",
    "                        img,\n",
    "                        pixel_thresholds=[300,1500]):\n",
    "    labels = measure.label(mask, neighbors=8, background=0)\n",
    "    new_mask = np.zeros(mask.shape, dtype='uint8')\n",
    "    large_mask = np.zeros(mask.shape, dtype='uint8')\n",
    "    ltwh = []\n",
    "    centerpoints = []\n",
    "    sizes = []\n",
    "    # loop over the unique components\n",
    "    for label in np.unique(labels):\n",
    "        # if this is the background label, ignore it\n",
    "        if label == 0:\n",
    "            continue\n",
    "        # otherwise, construct the label mask and count the\n",
    "        # number of pixels \n",
    "        labelMask = np.zeros(mask.shape, dtype=\"uint8\")\n",
    "        labelMask[labels == label] = 255\n",
    "        numPixels = cv2.countNonZero(labelMask)\n",
    "        \n",
    "        # if the number of pixels in the component is sufficiently\n",
    "        # large, then add it to our mask of \"large blobs\"\n",
    "        if numPixels > pixel_thresholds[0]:\n",
    "            sizes.append(numPixels)\n",
    "            if numPixels > pixel_thresholds[1]:\n",
    "                large_mask = cv2.add(large_mask, labelMask)\n",
    "                continue\n",
    "            new_mask = cv2.add(new_mask, labelMask)\n",
    "            y, x = np.where(labels == label)\n",
    "            left, top = np.min(x), np.min(y)\n",
    "            width, height = (np.max(x) - left), (np.max(y) - top)\n",
    "            centerpoint = left + 0.5 * width, top + 0.5 * height\n",
    "            ltwh.append((left, top, np.max(x) - left, np.max(y) - top))\n",
    "            centerpoints.append(centerpoint)\n",
    "    \n",
    "    watershed = watershed_mask(large_mask, img)\n",
    "    \n",
    "    for label in np.unique(watershed[0]):\n",
    "        # if this is the background label, ignore it\n",
    "        if label == -1 or label == 1:\n",
    "            continue\n",
    "        # otherwise, construct the label mask and count the\n",
    "        # number of pixels \n",
    "        labelMask = np.zeros(mask.shape, dtype=\"uint8\")\n",
    "        labelMask[watershed[0] == label] = 255\n",
    "        numPixels = cv2.countNonZero(labelMask)\n",
    "        \n",
    "        if numPixels > pixel_thresholds[0]:\n",
    "            sizes.append(numPixels)\n",
    "            new_mask = cv2.add(new_mask, labelMask)\n",
    "            y, x = np.where(watershed[0] == label)\n",
    "            left, top = np.min(x), np.min(y)\n",
    "            width, height = (np.max(x) - left), (np.max(y) - top)\n",
    "            centerpoint = left + 0.5 * width, top + 0.5 * height\n",
    "            ltwh.append((left, top, np.max(x) - left, np.max(y) - top))\n",
    "            centerpoints.append(centerpoint)\n",
    "\n",
    "    return new_mask, np.array(ltwh), np.array(centerpoints), np.array(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_bboxes(cropped_imgs, \n",
    "              bboxes, \n",
    "              orig_size=(1536,1536),\n",
    "              size=(256,256)):\n",
    "    drawn = []\n",
    "    for cropped_img, bbox in zip(cropped_imgs, bboxes):\n",
    "        cropped_img = cropped_img.copy()\n",
    "        # recalculate coordinates based on crop\n",
    "        # get the x,y of the centerpoint of the bounding box\n",
    "        box_center_x = bbox[0] + 0.5 * bbox[2]\n",
    "        box_center_y = bbox[1] + 0.5 * bbox[3]\n",
    "\n",
    "        # ensure that the bounding box of the desired size\n",
    "        # stays within the shape of the image\n",
    "        centerpoint_x = np.clip(box_center_x, \n",
    "                                 a_min = size[0] / 2,\n",
    "                                 a_max = orig_size[0] - size[0] / 2)\n",
    "        centerpoint_y = np.clip(box_center_y, \n",
    "                                 a_min = size[1] / 2,\n",
    "                                 a_max = orig_size[1] - size[1] / 2)\n",
    "\n",
    "        # cast to int\n",
    "        x_crop = int(centerpoint_x - 0.5 * size[0])\n",
    "        y_crop = int(centerpoint_y - 0.5 * size[1])\n",
    "\n",
    "        x1 = bbox[0] - x_crop\n",
    "        y1 = bbox[1] - y_crop\n",
    "        x2 = bbox[0] + bbox[2] - x_crop\n",
    "        y2 = bbox[1] + bbox[3] - y_crop\n",
    "\n",
    "        #print(x1, y1, x2, y2)\n",
    "        cv2.rectangle(cropped_img, (x1, y1), (x2, y2),(0, 0, 255), 1)\n",
    "        drawn.append(cropped_img)\n",
    "    return drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_imageset(imagepath_list,\n",
    "                  blobs_dir,\n",
    "                  negatives_dir,\n",
    "                  blobs_bboxes,\n",
    "                  negatives=True,\n",
    "                  rescale_factor=2,\n",
    "                  rescale_dims=(768, 768),\n",
    "                  thresholds=[100, 1600],\n",
    "                  negative_details=SAVE_DIR+'negative_details.csv',\n",
    "                  image_details=SAVE_DIR+'image_details.csv'):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for imagepath in tqdm(imagepath_list):\n",
    "        img =  cv2.imread(imagepath)\n",
    "        img_resized = cv2.resize(img, rescale_dims)\n",
    "        # apply hue mask\n",
    "        hue_mask = hsv_mask(img_resized,\n",
    "                            show=False)\n",
    "        # clean up mask\n",
    "        cleaned_mask = clean_mask(hue_mask)\n",
    "        filtered, bboxes, centerpoints, sizes = threshold_and_bound(cleaned_mask, img_resized,\n",
    "                                                                pixel_thresholds=thresholds)\n",
    "        if negatives:\n",
    "            negative_bboxes = get_negative_bboxes(cleaned_mask)\n",
    "            negative_bboxes = rescale_factor * negative_bboxes # rescale to full size\n",
    "            #show_bboxes(img, negative_bboxes)\n",
    "            cropped_negatives, negative_coords = crop_from_img(img, \n",
    "                                                              negative_bboxes,\n",
    "                                                              size=(256,256))\n",
    "            save_cropped(cropped_negatives,\n",
    "                         imagepath,\n",
    "                         negatives_dir,\n",
    "                         append='negative')\n",
    "            negative_sizes = np.zeros(len(negative_coords))\n",
    "            write_details_to_csv(negative_details,\n",
    "                                 negative_coords,\n",
    "                                 negative_bboxes,\n",
    "                                 imagepath,\n",
    "                                 negative_sizes)\n",
    "            # save information\n",
    "        bboxes = rescale_factor * bboxes\n",
    "        centerpoints = rescale_factor * centerpoints\n",
    "        sizes = rescale_factor ** 2 * sizes\n",
    "        #show_bboxes(img, bboxes)\n",
    "        cropped_images, cropped_coords = crop_from_img(img, \n",
    "                                                       bboxes, \n",
    "                                                       size=(256,256))\n",
    "        save_cropped(cropped_images, imagepath, blobs_dir)\n",
    "        drawn_bboxes = draw_bboxes(cropped_images, bboxes)\n",
    "        save_cropped(drawn_bboxes, imagepath, blobs_bboxes)\n",
    "        write_details_to_csv(image_details,\n",
    "                             cropped_coords,\n",
    "                             bboxes,\n",
    "                             imagepath,\n",
    "                             sizes)\n",
    "        # save information\n",
    "    return bboxes, centerpoints, cropped_coords, sizes\n",
    "\n",
    "def save_cropped(cropped_images,\n",
    "                 imagepath,\n",
    "                 outdir,\n",
    "                 append='',):\n",
    "    for i, image in enumerate(cropped_images):\n",
    "        path = os.path.splitext(imagepath)\n",
    "        split = path[0].split('/')\n",
    "        col = split[-1]\n",
    "        row = split[-2]\n",
    "        wsi = split[-4]\n",
    "        dirname = os.path.join(outdir, wsi)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        filename = \"_\".join([wsi, row, col, str(i)]) + '.jpg'\n",
    "        cv2.imwrite(dirname + \"/\" + filename, image)\n",
    "        \n",
    "def save_image_details(dataframe,\n",
    "                       cropped_images,\n",
    "                       bboxes,\n",
    "                       centerpoints,\n",
    "                       imagepath,\n",
    "                       outdir):\n",
    "    for i, (image, bbox) in enumerate(zip(cropped_images, bboxes)):\n",
    "        path = os.path.splitext(imagepath)\n",
    "        split = path[0].split('/')\n",
    "        col = split[-1]\n",
    "        row = split[-2]\n",
    "        wsi = split[-4]\n",
    "        coords = bbox\n",
    "        dirname = os.path.join(outdir, wsi)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        filename = \"_\".join([wsi, row, col, i]) + '.jpg'\n",
    "        cv2.imwrite(dirname + \"/\" + filename, image)\n",
    "\n",
    "\n",
    "def write_details_to_csv(filename,\n",
    "                         cropped_coords,\n",
    "                         bboxes,\n",
    "                         imagepath,\n",
    "                         sizes):\n",
    "    details = []\n",
    "    path = os.path.splitext(imagepath)\n",
    "    split = path[0].split('/')\n",
    "    col = split[-1]\n",
    "    row = split[-2]\n",
    "    source = split[-4]\n",
    "    for i, (img_coords, blob_coords, size) in enumerate(zip(cropped_coords,\n",
    "                                                            bboxes,\n",
    "                                                            sizes)):\n",
    "        imagename = \"_\".join([source, row, col, str(i)]) + '.jpg'\n",
    "        image_details = [imagename,\n",
    "                         source,\n",
    "                         col,\n",
    "                         row,\n",
    "                         img_coords,\n",
    "                         blob_coords,\n",
    "                         size]\n",
    "        details.append(image_details)\n",
    "\n",
    "    with open(filename, 'a') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(details)\n",
    "    return details\n",
    "\n",
    "def init_csvs(filenames):\n",
    "    data_fields = [['imagename',\n",
    "                   'source',\n",
    "                   'tile_column',\n",
    "                   'tile_row',\n",
    "                   'image coordinates (xywh)',\n",
    "                   'blob coordinates (xywh)',\n",
    "                   'blob size']]\n",
    "\n",
    "    # Create CSVs for Image Details\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        with open(filename, \"w\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = glob.glob(IMG_DIR  + '*/0/*/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/32056 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kangway/data/cnn_path/opencv_blobs/03_22_18/negative_details.csv\n",
      "/home/kangway/data/cnn_path/opencv_blobs/03_22_18/image_details.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32056/32056 [1:28:32<00:00,  6.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbfeb61978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_csvs([SAVE_DIR + 'negative_details.csv',\n",
    "           SAVE_DIR + 'image_details.csv'])\n",
    "\n",
    "crop_imageset(images,\n",
    "              BLOBS_DIR,\n",
    "              NEGATIVES,\n",
    "              IMG_BBOXES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
